(dp0
S'training_loss'
p1
(lp2
F497.8563232421875
aF170.67649841308594
aF159.4445343017578
aF149.33984375
aF136.21116638183594
aF127.15628051757812
aF122.74420928955078
aF118.83816528320312
aF106.35377502441406
aF103.43599700927734
aF97.63887023925781
aF93.98934173583984
aF89.82357788085938
aF87.23052978515625
aF83.33182525634766
aF80.59696960449219
aF78.93915557861328
aF78.57804107666016
aF76.2004165649414
aF76.13150024414062
aF74.12891387939453
aF69.63363647460938
aF68.20186614990234
aF66.22528839111328
aF69.46208953857422
aF62.849910736083984
aF64.32563018798828
aF62.771270751953125
aF64.59552764892578
aF58.96690368652344
aF60.68675231933594
asS'validation_loss'
p3
(lp4
F482.08575439453125
aF167.57785034179688
aF159.09764099121094
aF151.66453552246094
aF146.57965087890625
aF138.49778747558594
aF134.8974609375
aF129.690673828125
aF125.21340942382812
aF123.21368408203125
aF122.34528350830078
aF120.51396942138672
aF121.21002960205078
aF116.71527099609375
aF119.38126373291016
aF114.87267303466797
aF116.71983337402344
aF116.90666198730469
aF115.34407806396484
aF115.94601440429688
aF114.51296997070312
aF114.16211700439453
aF114.09222412109375
aF113.34410095214844
aF117.27713012695312
aF112.63578033447266
aF115.58296203613281
aF113.61141204833984
aF115.82439422607422
aF112.32545471191406
aF111.97093200683594
as.