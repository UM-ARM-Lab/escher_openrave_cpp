(dp0
S'training_loss'
p1
(lp2
F433.3180236816406
aF140.5164031982422
aF139.9093017578125
aF136.428955078125
aF136.3083953857422
aF133.04031372070312
aF129.32015991210938
aF134.51751708984375
aF118.13624572753906
aF111.66455841064453
aF109.35640716552734
aF105.63001251220703
aF102.90691375732422
aF100.13150787353516
aF97.61774444580078
aF94.4036865234375
aF93.18341827392578
aF98.66372680664062
aF90.1321029663086
aF90.72237396240234
aF88.24241638183594
aF89.45164489746094
aF94.1695785522461
aF85.9670639038086
aF84.09123229980469
aF82.53583526611328
aF88.4852066040039
aF81.88224792480469
aF80.94014739990234
aF79.69789123535156
aF78.60494232177734
asS'validation_loss'
p3
(lp4
F428.0821838378906
aF135.90988159179688
aF136.25025939941406
aF131.9833221435547
aF134.42694091796875
aF130.68499755859375
aF128.5149688720703
aF133.2390594482422
aF121.46315002441406
aF119.46696472167969
aF119.76853942871094
aF116.91899871826172
aF116.43927001953125
aF116.62519836425781
aF114.77719116210938
aF114.65652465820312
aF113.51175689697266
aF119.89310455322266
aF114.47567749023438
aF116.51815795898438
aF115.17069244384766
aF116.37112426757812
aF119.60842895507812
aF114.88053131103516
aF114.51824188232422
aF113.34049987792969
aF117.8780517578125
aF112.5028076171875
aF115.2791519165039
aF116.94928741455078
aF113.73861694335938
as.