(dp0
S'training_loss'
p1
(lp2
F491.73260498046875
aF235.95045471191406
aF209.69822692871094
aF191.62371826171875
aF163.5294189453125
aF147.42404174804688
aF133.9664764404297
aF128.50662231445312
aF125.73074340820312
aF121.20899963378906
aF110.46371459960938
aF108.5830078125
aF113.08580780029297
aF97.83673095703125
aF92.61058044433594
aF93.43229675292969
aF91.2188720703125
aF84.95542907714844
aF81.49398803710938
aF80.20474243164062
aF80.25263214111328
aF74.86690521240234
aF72.4583969116211
aF73.32352447509766
aF67.19696044921875
aF71.18428802490234
aF65.3751220703125
aF63.256103515625
aF63.9517936706543
aF63.03828048706055
aF59.706825256347656
asS'validation_loss'
p3
(lp4
F487.0162353515625
aF249.55551147460938
aF230.88661193847656
aF226.02182006835938
aF203.40069580078125
aF203.12445068359375
aF193.68630981445312
aF217.512451171875
aF191.27305603027344
aF191.61476135253906
aF200.34393310546875
aF194.68934631347656
aF198.23548889160156
aF217.8338165283203
aF213.7363739013672
aF198.14126586914062
aF197.07704162597656
aF208.1036834716797
aF217.41439819335938
aF212.45602416992188
aF208.85055541992188
aF213.13021850585938
aF236.66635131835938
aF203.0694122314453
aF224.165283203125
aF207.47915649414062
aF213.6226043701172
aF218.64910888671875
aF233.28147888183594
aF212.19248962402344
aF214.68968200683594
as.