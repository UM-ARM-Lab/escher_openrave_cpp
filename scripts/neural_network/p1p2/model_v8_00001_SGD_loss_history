(dp0
S'training_loss'
p1
(lp2
F60.533409118652344
aF56.64994430541992
aF50.51229476928711
aF87.79220581054688
aF66.54293823242188
aF47.642982482910156
aF57.56170654296875
aF49.31249237060547
aF51.024932861328125
aF44.46941375732422
aF48.41965103149414
aF48.526554107666016
aF62.556304931640625
aF41.02043533325195
aF45.53327178955078
aF37.727169036865234
aF63.705841064453125
aF46.39878463745117
aF46.72483444213867
aF32.09846115112305
aF44.659671783447266
aF41.03412628173828
aF49.792030334472656
aF33.343841552734375
aF32.35712432861328
aF30.48284149169922
aF41.65081024169922
aF27.514320373535156
aF39.961971282958984
aF25.87804412841797
asS'validation_loss'
p3
(lp4
F122.37743377685547
aF122.10977935791016
aF117.46546173095703
aF139.0072021484375
aF127.48775482177734
aF119.31184387207031
aF126.29454803466797
aF118.22068786621094
aF122.37725830078125
aF121.33407592773438
aF121.45330047607422
aF120.00432586669922
aF127.77162170410156
aF120.66973114013672
aF123.12844848632812
aF118.97733306884766
aF130.28109741210938
aF120.81011962890625
aF123.55735778808594
aF117.4249038696289
aF120.67532348632812
aF120.42655944824219
aF123.8868408203125
aF118.37345123291016
aF118.68819427490234
aF119.802734375
aF119.63622283935547
aF116.62982177734375
aF120.21727752685547
aF115.6517105102539
as.